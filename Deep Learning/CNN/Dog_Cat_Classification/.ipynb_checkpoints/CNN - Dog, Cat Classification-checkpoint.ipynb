{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847f054a",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "- Training set: 8000 images in total\n",
    "    - Dogs: 4000 images\n",
    "    - Cats: 4000 images\n",
    "\n",
    "- Test set: 2000 images in total\n",
    "    - Dogs: 1000 images\n",
    "    - Cats: 1000 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf1bef",
   "metadata": {},
   "source": [
    "# 1. Importing the Libraries\n",
    "\n",
    "- `tensorflow`\n",
    "\n",
    "- preprocessing module of `keras` (for image pre-processing)\n",
    "    - `tensorflow.keras.preprocessing.image` module has a `ImageDataGenerator` class &rarr; [docs](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38455354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d733b107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a86cc",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing\n",
    "\n",
    "- We preprocess the dataset of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fc703",
   "metadata": {},
   "source": [
    "## 2.1. Preprocessing the Training set\n",
    "\n",
    "- We will apply some transformations on all images of only the training set.\n",
    "    - To avoid overfitting.\n",
    "    - We apply some simple geometric transformations like transvections, rotate images, do some zooms, and so on.\n",
    "    - This will 'augment' the images.\n",
    "    - This is called 'Image Augmentation'.\n",
    "    - `ImageDataGenerator` class of `tensorflow.keras.preprocessing.image` module does this work.\n",
    "\n",
    "- `rescale` parameter does Feature Scaling for each pixel value (divides by 255 in our example).\n",
    "    - Feature Scaling is compulsory for NN.\n",
    "    \n",
    "- Refer to `tf.keras` documentation for details of this class and code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9302c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create the Training Image Data Generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255,\n",
    "    shear_range = 0.2, \n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "# connect it to the training set\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set', # path of training set\n",
    "    target_size = (64, 64), # final size of the images, which will be fed into CNN\n",
    "    batch_size = 32, # number of images in each batch\n",
    "    class_mode = 'binary' # can be binary or categorical\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5510f0",
   "metadata": {},
   "source": [
    "## 2.2. Preprocessing the Test set\n",
    "\n",
    "- For test images, we dont apply image augmentations. But, they should be rescaled (pixels).\n",
    "\n",
    "- for `predict` method, images must have same dimensions. So, make use of same `target_size` in training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4a1aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create the Test set Image Data Generator\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# connect it to the test set\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size = (64, 64), # should be of same size as in training set\n",
    "        batch_size = 32, \n",
    "        class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8ddc3",
   "metadata": {},
   "source": [
    "# 3. Building the CNN\n",
    "\n",
    "- Lets build the architecture of the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d67ab",
   "metadata": {},
   "source": [
    "## 3.1. Initialising the CNN\n",
    "\n",
    "- A CNN is still a sequence of layers, so make use of `Sequential` class.\n",
    "\n",
    "- Then, step by step, add layers to the `cnn` model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d1352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e36ceb",
   "metadata": {},
   "source": [
    "### 3.1. Step 1 - Convolution\n",
    "\n",
    "- Convolution Layer is an instance of `tensorflow.keras.layers` module's `Conv2D` class.\n",
    "\n",
    "- Constructor arguments:\n",
    "    - Filter or Feature Detector or Kernel: `filters`\n",
    "        - Classic Architecture: 32 filters in first conv layers, and 32 filters in 2nd conv layer\n",
    "    - Kernel Size: `kernel_size` (Size of feature detector. Say 3 &rarr; its a 3*3 matrix)\n",
    "    - activation function: `activation`\n",
    "    - `input_shape` &rarr; while adding any layers, you should specify the input size.\n",
    "        - Color images &rarr; RGB &rarr; 3D input shape.\n",
    "        - Since images are resized to 64*64 px &rarr; `[64, 64, 3]` is the input shape.\n",
    "        - This is only for the input layer (the 1st conv layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a41b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "cnn.add(\n",
    "    Conv2D(\n",
    "        filters = 32,\n",
    "        kernel_size = 3, # 3*3\n",
    "        activation = 'relu',\n",
    "        input_shape = [64, 64, 3]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14917e1",
   "metadata": {},
   "source": [
    "### 3.1. Step 2 - Pooling\n",
    "\n",
    "- Lets apply max pooling.\n",
    "\n",
    "- Its actually a pooling layer.\n",
    "\n",
    "- `tensorflow.keras.layers` has a `MaxPool2D` class.\n",
    "\n",
    "- 2 args:\n",
    "    - `pool_size` (that square frame width)\n",
    "        - Recommended pool size in max pooling is 2\n",
    "    - `strides` (jump size of the frame)\n",
    "        - Recommended stride is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e67af3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPool2D\n",
    "\n",
    "cnn.add(\n",
    "    MaxPool2D(\n",
    "        pool_size = 2,\n",
    "        strides = 2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969bd49b",
   "metadata": {},
   "source": [
    "## 3.2. Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa639f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 2nd conv layer\n",
    "cnn.add(\n",
    "    Conv2D(\n",
    "        filters = 32,\n",
    "        kernel_size = 3, # 3*3\n",
    "        activation = 'relu',\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn.add(\n",
    "    MaxPool2D(\n",
    "        pool_size = 2,\n",
    "        strides = 2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba42265",
   "metadata": {},
   "source": [
    "### 3.2. Step 3 - Flattening\n",
    "\n",
    "- Flatten the result into a 1D vector which will be the input to an ANN.\n",
    "\n",
    "- Flattening layer\n",
    "    - `tensorflow.keras.layers` module has a `Flatten` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c74cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257356a",
   "metadata": {},
   "source": [
    "### 3.2. Step 4 - Full Connection\n",
    "\n",
    "- Add a fully connected (hidden) layer: `Dense` layer\n",
    "\n",
    "- For Computer Vision applications, use some bigger number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698e9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "cnn.add(\n",
    "    Dense(\n",
    "        units = 128,\n",
    "        activation = 'relu'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed0449",
   "metadata": {},
   "source": [
    "### 3.2. Step 5 - Output Layer\n",
    "\n",
    "- Fully connected to the hidden layer\n",
    "\n",
    "- Number of units in output layer = 1 for binary classification and use sigmoid activation\n",
    "\n",
    "- Number of units in output layer = number of output classes and use softmax activation for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ca68069",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(\n",
    "    Dense(\n",
    "        units = 1,\n",
    "        activation = 'sigmoid'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698df8b",
   "metadata": {},
   "source": [
    "# 4. Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c77cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               802944    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get a summary of the model so far\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a272e7",
   "metadata": {},
   "source": [
    "- we do a different kind of training.\n",
    "\n",
    "- lets train the NN for 25 epochs.\n",
    "    - for each epoch, we will see the test set results\n",
    "    \n",
    "- each epoch takes some considerable time to execute. so try different values until you see some sort of convergence\n",
    "    \n",
    "- Adam optimizer to perform stochastic gradient descent to update weights\n",
    "\n",
    "- BinaryCrossEntropy loss function\n",
    "\n",
    "- accuracy metrics to measure the performance\n",
    "\n",
    "- training takes lot of time, depending on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1be98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "cnn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f994fe26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 112s 434ms/step - loss: 0.6755 - accuracy: 0.5724 - val_loss: 0.6225 - val_accuracy: 0.6505\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.6204 - accuracy: 0.6536 - val_loss: 0.6538 - val_accuracy: 0.6345\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 0.5896 - accuracy: 0.6930 - val_loss: 0.5729 - val_accuracy: 0.7125\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 37s 150ms/step - loss: 0.5557 - accuracy: 0.7125 - val_loss: 0.5183 - val_accuracy: 0.7445\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.5200 - accuracy: 0.7476 - val_loss: 0.5328 - val_accuracy: 0.7465\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.4984 - accuracy: 0.7594 - val_loss: 0.4738 - val_accuracy: 0.7805\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4735 - accuracy: 0.7747 - val_loss: 0.4960 - val_accuracy: 0.7615\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.4551 - accuracy: 0.7870 - val_loss: 0.4717 - val_accuracy: 0.7860\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 36s 146ms/step - loss: 0.4419 - accuracy: 0.7926 - val_loss: 0.4691 - val_accuracy: 0.7910\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.4190 - accuracy: 0.8045 - val_loss: 0.4678 - val_accuracy: 0.7840\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 36s 146ms/step - loss: 0.4044 - accuracy: 0.8146 - val_loss: 0.4603 - val_accuracy: 0.7920\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.3932 - accuracy: 0.8238 - val_loss: 0.4675 - val_accuracy: 0.7725\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.3749 - accuracy: 0.8295 - val_loss: 0.4504 - val_accuracy: 0.7970\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 0.3539 - accuracy: 0.8397 - val_loss: 0.4628 - val_accuracy: 0.8030\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.3478 - accuracy: 0.8410 - val_loss: 0.4500 - val_accuracy: 0.8095\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.3205 - accuracy: 0.8583 - val_loss: 0.4908 - val_accuracy: 0.7945\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.3025 - accuracy: 0.8679 - val_loss: 0.5377 - val_accuracy: 0.7850\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.2914 - accuracy: 0.8758 - val_loss: 0.4793 - val_accuracy: 0.8105\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.2777 - accuracy: 0.8829 - val_loss: 0.5426 - val_accuracy: 0.7810\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.2538 - accuracy: 0.8904 - val_loss: 0.4955 - val_accuracy: 0.7945\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.2387 - accuracy: 0.9025 - val_loss: 0.5099 - val_accuracy: 0.8145\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.2300 - accuracy: 0.9047 - val_loss: 0.5516 - val_accuracy: 0.7935\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.2283 - accuracy: 0.9065 - val_loss: 0.5309 - val_accuracy: 0.8150\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.2079 - accuracy: 0.9159 - val_loss: 0.5525 - val_accuracy: 0.7975\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1870 - accuracy: 0.9265 - val_loss: 0.5956 - val_accuracy: 0.7990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b819d50dc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the cnn, evaluate test set at each epoch\n",
    "cnn.fit(\n",
    "    x = training_set,\n",
    "    validation_data = test_set, # on which we want to evaluate and validate\n",
    "    epochs = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f0e1c",
   "metadata": {},
   "source": [
    "# 5. Making a single prediction\n",
    "\n",
    "- import `numpy` library.\n",
    "\n",
    "- import `tensorflow.keras.preprocessing.image` module\n",
    "\n",
    "- load the image, which acts as input to `predict` method\n",
    "    - input image should have same size as the images used in training\n",
    "    - `image.load_image(path_to_image_from_root_with_extension, target_size = input_img_size)`\n",
    "    \n",
    "- this returns a `PIL` format. `predict` method requires an array. so, do that conversion\n",
    "\n",
    "- then, since we created batches of 32 images during cnn training. so, all images during predictions should be in batches\n",
    "    - dimension of the batch is always the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b81fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# load the image (PIL)\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "\n",
    "# convert into array (PIL -> numpy array)\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# add an extra dimension (1st dimension -> axis = 0), converting the image into a batch\n",
    "test_image = np.expand_dims(test_image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002d3a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "[[1.]]\n",
      "{'cats': 0, 'dogs': 1}\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "# predict the results (0 or 1)\n",
    "result = cnn.predict(test_image)\n",
    "print(result)\n",
    "\n",
    "# encode the results (what does 0 repr? dog or cat?)\n",
    "print(training_set.class_indices)\n",
    "\n",
    "# display the result in a good format\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Dog'\n",
    "else:\n",
    "    prediction = 'Cat'\n",
    "    \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3be1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "[[5.929237e-38]]\n",
      "{'cats': 0, 'dogs': 1}\n",
      "Cat\n"
     ]
    }
   ],
   "source": [
    "# TESTING FOR CAT\n",
    "\n",
    "# load the image (PIL)\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "\n",
    "# convert into array (PIL -> numpy array)\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# add an extra dimension (1st dimension -> axis = 0), converting the image into a batch\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "# predict the results (0 or 1)\n",
    "result = cnn.predict(test_image)\n",
    "print(result)\n",
    "\n",
    "# encode the results (what does 0 repr? dog or cat?)\n",
    "print(training_set.class_indices)\n",
    "\n",
    "# display the result in a good format\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Dog'\n",
    "else:\n",
    "    prediction = 'Cat'\n",
    "    \n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
